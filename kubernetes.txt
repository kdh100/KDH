1. Kubernetes(K8s) 개요
    * 컴퓨터 클러스터에 어플리케이션 컨테이너의 배치(스케줄링) 및 실행을 오케스트레이션하는 운영 수준 플랫폼
    * 공식 사이트
        - Kubernetes : https://kubernetes.io/
        - CNCF(Cloud Native Computing Foundation) : https://www.cncf.io/
    * 특징
        - 스냅샷을 이용한 롤백/롤아웃
        - 서비스 검색 및 로드밸런싱
        - 일괄 실행 관리
        - 자가 치유(Health-Check 를 통한 상태 확인/복구)
        - 수평적 스케일링
    * Kubernetes 클러스터 구성
        - {Control Plane(Master) { Node(Worker) {Container(Pod)}}}
        - Control Plane(Master)
            # kube-apiserver
                ~ API 를 통한 인스턴스 배포/실행, 트래픽 조정
                ~ frontend
            # etcd
                ~ 키:값 저장소(KVS; Key-Value Store)
                ~ backend
            # kube-scheduler
                ~ 파드 감지, 노드 배정, 노드 실행
            # kube-controller-manager
                ~ 프로세스(노드, 레플리케이션, 엔드포인트(서비스<->파드 연결), 계정/토큰) 관리
                ~ health-check
            # cloud-controller-manager
                ~ 클라우드 전용 API 프로세스(노드, 라우팅, 서비스) 관리
        - Node(Worker)
            # kubelet
                ~ 데몬처럼 파드에서 컨테이너 동작 관리
            # kube-proxy
                ~ 네트워크 규칙 유지
            # container-runtime(container-engine)
                ~ 컨테이너 실행 담당 소프트웨어
        - Add-on
            # DNS
                ~ 컨테이너 내부 파드끼리의 통신을 위해 필요
            # WEB-UI(대시보드)
                ~ 클러스터 및 동작하는 어플리케이션 관리, Troubleshooting 지원
            # container resource monitoring
                ~ 리소스 기록/열람 UI 제공
            # cluster-level logging
                ~ 로그 저장, 검색/열람 인터페이스 제공
            # CNI(Container Network Interface)
                ~ SDN
    * 어플리케이션 구성
        - 파드(Pod)
            # 여러개의 컨테이너를 모아 관리(작성/시작/정지/삭제)
                ~ 웹 서버와 DB 서버 같이 역할이 다른 컨테이너를 하나의 파드에 저장하면 안됨
        - 레플리카셋(ReplicaSet)
            # 미리 지정된 파드를 작성하여 실행하는 템플릿
                ~ 10개 지정하면 9개 가동 시 1개 추가 가동, 11개 가동 시 1개 중지 등
        - 전개(Deployment)
            # 레플리카셋 생성
                ~ 레플리카셋 이력 관리용
    * 네트워크 관리
        - 서비스(service)
            # 외부 --> 클러스터 내부 파드에 엑세스하는 서비스 정의
                ~ 로드밸런서 : IP(Cluster IP(사설), External IP(공인)) + 포트 번호에 대한 L4 레벨 부하 분산
                ~ 인그레스(Ingress) : 서비스와 연결되어 통신 내용을 프록시
        - 라벨(Label)
            # 리소스를 적절히 관리하기 힘들기 때문에 식별하기 쉬운 Label 을 내부에서 랜덤 부여/관리
                ~ key:value 형식의 임의 문자열
2. GCP(Google Cloud Platform)를 이용한 GKE(Google GKE) 사용
    * GCP 구성
        <GCP 프로젝트 ID 확인/설정>
        - https://cloud.google.com -> 프로젝트 이름/ID 확인 -> 프롬프트 열기
            # PROJECT_ID=$(gcloud config list project --format "value(core.project)")
            # echo $PROJECT_ID
            # gcloud config set project $PROJECT_ID
            # sudo -l
        <GCP registry 설정; Cloud Source Repositories API>
        - GCP 콘솔 -> API 및 서비스 - 라이브러리 -> Cloud Source Repositories API 사용
        - GCP 프롬프트
            # git clone https://github.com/asashiho/dockertext2
            # git config --global \
            # credential.'https://source.developers.google.com'.helper gcloud.sh
            # git config --list
            # gcloud source repos create dockertext2
                ~ dockertext2 : 구글 레지스트리에 생성될 저장소 이름
            # git remote add google \
              https://source.developers.google.com/p/$PROJECT_ID/r/dockertext2
            # git push google master
                ~ 생성된 저장소(google, master 브랜치)에 디렉토리에 있는 파일들을 푸시
    * GCP 를 이용한 Docker 이미지 빌드
        <GCP API 설정>
        - GCP 콘솔 -> API 및 서비스 - 라이브러리
            # GKE, container registry, cloud build API 사용
                ~ 좌측 메뉴에 핀셋 설정으로 즐겨찾기 등록하면 편함
        <이미지 빌드>
        - GCP 프롬프트
            # cd ~/dockertext2/chap09
            # cat config/cloudbuild.yaml
            # vi config/configmap.yaml
                ~ 'project.id: ' 부분에 본인의 프로젝트 ID 작성
            # gcloud builds submit --config config/cloudbuild.yaml
                ~ 간혹 신규 서비스 사용으로 인해 접근 거부로 인한 오류로 정상적으로 등록 안될 수 있음
                ~ 잠시 기다렸다가 여러번 재실행 하면 정상 동작함(인증 과정에 시간이 오래 소요될 수 있음)
    * GCP 를 이용한 Kubernetes 클러스터 구축
        <클러스터 생성>
        - GCP 콘솔 -> GKE -> 클러스터 -> 만들기 -> Standard -> 구성
            # 이름: imageview, 영역: asia-northeast1-a, default-pool: {크기: 3}
                ~ 선택 후 만들기(시간 좀 걸림)
        <클러스터 관리 인증 정보 생성>
        - GCP 프롬프트
            # gcloud container cluster get-credentials imageview --zone=asia-northeast1-a
            # cat ~/.kube/config
            # kubectl get nodes [-o yaml]
            # echo 'source <(kubectl completion bash)' >>~/.bash_profile
            # source ~/.bash_profile
                ~ kubernetes 관리 명령어 자동완성 기능 활성화
    * kubectl CMD
        - configmap : 어플리케이션에서 공통으로 사용하는 정보(plaintext)
            # kubectl create configmap <file> <source>
                ~ GCP -> GKE -> 보안 비밀 및 Configmap 에서 확인 가능
            # kubectl get configmaps [-o yaml]
            # kubectl delete configmaps <map-name>
        - secrets : API 키나 DB 연결을 위한 ID/PW 등의 기밀 데이터(base64 인코딩)
            # kubectl create -f ./secret.yaml
                ~ GCP -> GKE -> 보안 비밀 및 Configmap 에서 확인 가능
            # kubectl get secret <secret-name> [-o yaml]
            # kubectl delete secret <secret-name>
    *어플리케이션 배포
        <파드 가동/중지>
        - GCP 프롬프트
            # kubectl api-resources | grep deployment
            # vi config/deployment-blue|green.yaml --> 일부 내용 아래처럼 수정
                ~ apiVersion: apps/v1
                ~ spec:
                    selector:
                      matchLabels:
                        type: webserver
                  replicas: 3
                ~ spec:
                    containers:
                      image: gcr.io/vataltrick-123456/imageview:blue|green
            # kubectl create -f config/deployment-blue|green.yaml
            # (TERM2) watch kubectl get pods [-o wide]
            # kubectl delete pod webserver-blue-xxxxxxx-xxxxx
                ~ 강제로 파드 중지시켜보면 해당 ID 값의 파드는 사라지고 신규 파드가 바로 생성됨
                ~ 만들어진 템플릿(레플리카셋)에 따라 항상 동일한 파드 유지
        <서비스 가동>
        - GCP 프롬프트
            # cat config/service.yaml
            # kubectl create -f config/service.yaml
            # (TERM3) watch kubectl get services [-o wide]
                ~ 웹 콘솔에서 공인 IP 확인/테스트 전에는 pending 상태가 상당히 오래 유지됨
        - GCP 콘솔 -> GKE -> 서비스 및 수신 -> 세부정보 -> 공인 IP 확인 -> 접속 테스트
        <어플리케이션 버전업; Blue-Green Deployment>
        - 가동중인 서버, 어플리케이션을 갱신할 때 사용하는 방법 중 하나
            # 가동중인 기존 서버 <-- 새로운 서버를 추가 가동시켜 엑세스 시켜놓고 기존 서버 업데이트
                ~ 문제 발생 시 바로 이전 서버로 롤백 가능
        - GCP 콘솔 -> GKE -> 서비스 및 수신 -> 세부 정보 -> 수정 -> 저장
            # selector 의 color 부분을 변경(blue --> green)
        - GCP 프롬프트
            # kubectl edit services webserver
                ~ selector 의 color 부분을 변경(blue --> green)
        <스케쥴링; Linux crontab, at>
        - GCP 프롬프트
            # cat config/cronjob.yaml
            # kubectl create -f config/cronjob.yaml
            # kubectl get jobs [--watch]
            # kubectl delete -f config/cronjob.yaml
3. 클라우드 환경에서 운용
    * 시스템 운용 기초
        - 가용성 관리
            # Cold|Hot-Standby
            # Health-Check
            # Load-Balancing
        - 시스템 감시
            # 머신 활동 감시
            # 리소스 감시
            # 잡 감시
            # 장애 대응 및 퍼포먼스 튜닝
    * GKE 를 사용한 Docker 환경 운용
    [상태 확인]
        <클러스터 상태 확인>
        - GCP 콘솔 -> GKE -> 세부정보 확인
        - GCP 프롬프트
            # gcloud container clusters list
            # kubectl cluster-info [dump]
                ~ dump 사용 시 자세한 정보 확인 가능
        <노드 확인>
        - GCP 콘솔 -> GKE -> 세부정보 -> 노드 탭
        - GCP 프롬프트
            # kubectl get nodes [-o wide]
        <파드 확인>
        - GCP 콘솔 -> GKE -> 작업 부하
        - GCP 프롬프트
            # kubectl get pods [-o wide]
            # kubectl describe pods <파드 ID>
        <서비스 확인>
        - GCP 콘솔 -> GKE -> 서비스 및 수신
        - GCP 프롬프트
            # kubectl get services [-o wide]
    [관리]
        <파드 관리>
        - GCP 프롬프트
            # kubectl get pods
            # kubectl scale --replicas=2 -f config/deployment-green.yaml
                ~ 레플리카셋의 파드를 2개로 설정
            # kubectl get pods
        <노드 관리>
        - GCP 프롬프트
            # gcloud container clusters list
            # gcloud continaer clusters resize imageview --num-nodes 5 --zone asia-northeast1-a
            # gcloud container clusters list
            # kubectl scale --replicas=5 -f config/deployment-green.yaml
            # kubectl get pods
                ~ 늘어난 노드에 맞춰 파드를 늘려본 수 다시 노드를 감소시키면
            # gcloud continaer clusters resize imageview --num-nodes 3 --zone asia-northeast1-a
            # kubectl get nodes
            # kubectl get pods
                ~ 다른 노드에 파드가 자동으로 위치 조정됨
    [리소스 작성/삭제/변경]
        <리소스 작성>
        - GCP 프롬프트
            # kubectl create -f <file> <source>
            # kubectl apply -f <file>
                ~ 기존 생성된 파일의 내용을 변경할 때
            # kubectl edit -f <file>
                ~ 현재 적용중인 리소스 파일이 저장된 위치(etcd)에서 직접 수정/적용할 때
            # kubectl delete -f <file>
    [클러스터 업그레이드/다운그레이드]
        - GCP 프롬프트
            # gcloud container clusters list
            # gcloud container get-setver-config --zone=asia-northeast1-a
            # gcloud container clusters upgrade imageview --cluster-version=<버전>
                ~ 마스터,노드 보다 더 높은 버전으로 업그레이드 하는 것은 불가능
    [클러스터 삭제; GCP 콘솔에서 진행]
        <서비스 삭제>
        - GKE -> 서비스 수신 -> webserver 선택 -> 삭제
        <노드 삭제>
        - GKE -> 작업부하 -> 모두 선택 -> 삭제
        <클러스터 삭제>
        - GKE -> imageview 선택 -> 삭제
        <GCP 서비스 종료>
        - API 및 서비스 -> 사용 설정된 API.. -> 서비스 선택 -> 사용 중지
            # Cloud Source Repositories API
            # Container Registry API
                ~ 위 2개의 서비스 사용 중지 시 GKE 자동 사용 중지 
        <프로젝트 삭제>
        - IAM 및 관리자 -> 리소스 관리 -> 프로젝트 선택 -> 삭제
4. 온프레미스 환경에서 운용
    [MiniKube 설치]
        <리눅스 가상환경 설치>
        - VM : VMWare
        - OS : CentOS
        - CPU : 2
        - MEM : 4 G
        - HDD : 40 GB
        - Hostname : 
            # controlplane : master.example.com
            # node(1,2,3) : node1|2|3.example.com
            # backup : minikube.example.com
                ~ hostnamectl set-hostname <hostname>
                ~ 변경 후 로그아웃 > 로그인
        - IP : 
            # controlplane : 192.168.10.10/24
            # node(1,2,3) : 192.168.10.{20|30|40}/24
            # backup : 192.168.10.50/24
                ~ nm-connection-editor 작업 후 nmcli connection up eth0
        - /etc/hosts
            # controlplane, nodes, backup 에 대한 내용 설정
        - SELinux
            # sestatus > disable or permissive
        - Firewall
            # systemctl disable --now firewalld
        - NTP
            # controplane(NTP Server)
                ~ vi /etc/chrony.conf > #allow 192.168.0.0/16 --> allow 192.168.10.0/24
                ~ systemctl restart chronyd
            # nodes, backup(NTP Client)
                ~ vi /etc/chrony.conf > server, pool --> #server, #pool
                                      > echo 'server 192.168.10.10 iburst' >> /etc/chrony.conf
                ~ systemctl restart chronyd
                ~ chronyc sources -v
        <MiniKube 설치>
        - 참고 URL : https://minikube.sigs.k8s.io/docs/tutorials/
        - backup 서버에서 구축
        - Docker 설치
            # cd
            # yum remove -y runc && \
              curl -fsSL https://get.docker.com -o get-docker.sh && \
              sh get-docker.sh && \
              systemctl enable --now docker
        - Minikube 설치
            # cd
            # curl -LO \
              https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
            # sudo install minikube-linux-amd64 /usr/local/bin/minikube
        - Minikube 실행
            # minikube start --force --memory=2048mb
                ~ --force : 루트 사용자로 실행
                ~ --memory : 메모리 설정
            # curl -LO "https://dl.k8s.io/release/\
              $(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
            # sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
            # kubectl completion bash | sudo tee /etc/bash_completion.d/kubectl > /dev/null
        <Minikube 실행>
        - 대시보드 실행
            # minikube dashboard
                ~ 하단의 URL 을 복사하여 다른 터미널 창에서 firefox 백그라운드로 확인
        - 어플리케이션 배포
            # kubectl create deployment hello-minikube --image=k8s.gcr.io/echoserver:1.4
            # kubectl get deployments
            <서비스 개시1>
            # kubectl expose deployment hello-minikube --type=NodePort --port=8080
            # kubectl get services
            # minikube service hello-minikube
                ~ 기본 웹브라우저(default browser: firefox)에서 내용 확인
            <서비스 개시2>
            # kubectl port-forward service/hello-minikube 7080:8080
            # firefox http://localhost:7080 &
            <로드 밸런서 배포>
            # kubectl create deployment balanced --image=k8s.gcr.io/echoserver:1.4
            # kubectl get deployments
            # kubectl expose deployment balanced --type=LoadBalancer --port=8080
            # kubectl get services
            # minikube tunnel
                ~ 로드밸런싱을 위한 유효 라우팅 IP 생성
            # kubectl get services
            # firefox http://<external IP>:8080 &
                ~ 유효 라우팅 IP 를 통해서 접근 시 백앤드 컨테이너에 접근 가능
        - 클러스터 관리
            # minikube pause|unpause
            # minikube status
            # minikube addons list
                ~ 추가 설치 가능한 플러그인 카탈로그
            # minikube delete --all
                ~ 도커를 통해 생성된 쿠버네티스 컨테이너 및 하위 정보들을 삭제
            # minikube stop
                ~ minikube start --force 로 다시 시작 가능
                ~ docker ps -a 로 컨테이너(minikube) 확인 가능
    [k8s 직접 설치]
    * 준비사항
        - Linux 계열 OS, 2GB 이상 MEM, 2 Core 이상 CPU, 네트워크 연결, 고유 Hostname, MAC, SWAP 비활성화
            # uname -a
                ~ Centos 7,8,9
                ~ Ubuntu 16.04, 18.04, 20.04, 22.04
            # free -h
            # lscpu -e
            # ping 8.8.8.8
            # hostnamectl
            # swapoff -a (/etc/fstab)
        - Kubespray 설치
            # 앤서블(Ansible) 기반의 쿠버네티스 클러스터 배포 툴
            # 고가용성 구조
                ~ 별도의 로드밸런서가 필요 없이 자체적인 고가용성 구조가 구축되어있음
                ~ 각각의 노드가 서버 내부에 존재하는 nginx proxy 를 바라보고 있음
                ~ 노드 내부의 nginx proxy 는 마스터(controlplane)의 kube-apiserver 를 바라보고 있음
                ~ 마스터의 장애 감지는 Health Check 를 통해서 nginx 가 알아서 처리함
            # 필수사항
                ~ Ansible v2.9, Python v3.8 이상
                ~ Ansible-playbook 을 위한 Jinja v2.11 이상
                ~ Ansible-playbook 을 위한 인벤토리 서버들의 SSH 키 공유
                ~ 적절한 firewall 규칙 우선 정의 필요(kubespray 에 의해 관리되지 않음)
                ~ 루트 사용자가 아닌 일반 사용자로 실행될 경우 --become 필요
    * YAML 파일 편집을 위한 설정
        - vi ~/.bashrc
            # alias vi='/usr/bin/vim'
        - source ~/.bashrc
        - vi ~/.vimrc
            # syntax on
              autocmd FileType yaml setlocal ai nu ts=2 sw=2 et
              autocmd FileType python setlocal ai nu ts=2 sw=2 et
    * 인벤토리 서버들의 SSH 키 공유
        - ssh-keygen
        - ssh-copy-id master|node1|node2|node3
    * 파이썬 준비
        - yum install -y python38 && rm -f /usr/bin/python3 && ln -s /usr/bin/python3.8 /usr/bin/python3
        - python3 --version
    * PIP3, Jinja 준비
        - yum install -y python3-pip wget git vim sshpass & sleep 1 && \
          python3 -m pip install --upgrade pip && sleep 2 && pip install -U Jinja2
    * Kubespray 준비
        - cd
        - git clone https://github.com/kubernetes-sigs/kubespray
        - cd kubespray/
        - python3 -m pip install -r requirements.txt
        - cp -rfp inventory/sample inventory/mycluster
        - declare -a IPS=(192.168.10.10 192.168.10.20 192.168.10.30 192.168.10.40)
        - CONFIG_FILE=inventory/mycluster/hosts.yaml \
          python3 contrib/inventory_builder/inventory.py ${IPS[0]}
            # all 부분의 hosts 를 노드에 맞게 수정
            # children 부분의 kube_control_plane 의 hosts 를 master 로 수정
              kube_node 의 hosts 를 각각의 node 로 수정
              etcd 부분의 hosts 를 matser 로 수정
        - 설치 완료 후 로그아웃/로그인
        - kubectl version --output=yaml
        - kubectl cluster-info
        - kubectl get nodes -o wide
    * 확인 후 각 노드들의 스냅샷 저장
5. K8S 운영
    * CMD 자동 완성 기능 활성화
        - echo "source <(kubectl completion bash)" >> ~/.bashrc
          echo "source <(kubeamd completion bash)" >> ~/.bashrc
        - source ~/.bashrc
    * kubectl CMD
        <기본>
        - kubectl --help
        - kubectl api-resources
        <노드 정보 확인>
        - kubectl get nodes -o wide
        - kubectl describe nodes master
        <파드 실행>
        - kubectl run web --image=nginx --port=80
            # image 를 pulling 하고 서비스 포트를 80으로 지정하여 직접 실행
        - kubectl run web --image=nginx --port=80 --dry-run=client > web-pod.yml
            # 파드를 실행시키지 않고 실행시키기 위한 설정값을 .yml 파일로 생성
        - kubectl create -f web-pod.yml
            # .yml 파일을 통해 파드 실행
