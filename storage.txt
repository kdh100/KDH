Zadara Storage(VPSA)
    * 사용량 기반 요금 정책의 클라우드 형태 서비스(STaaS; Storage as a Service)
        - 각 서비스 당 최대 100개의 노드, 64 PB 용량 운용 가능
        - 물리적 스토리지 장비를 통합하여 가상 스토리지 장비를 생성
            # 고객 별 가상 스토리지의 스펙 조정 가능(CPU, MEM 등)
                ~ IOPS 효율성 및 최대화 기대 가능
            # H/A 기능 및 Control Core(Engine) 이중화(A-S) 기능 구현
    * 클라우드 형태 서비스
        - 단독(직연동; Direct) 클라우드 및 멀티 클라우드 연결 지원
            # B2C 부터 다중 CSP <-> 클라우드 간 연결 가능
    * 장비 스펙
        - 볼륨 당 최대 150,000 IOPS
        - Snapshot-Clone, Mirroring, Online-Migration 지원
    * 장비군
        - Storage Array
            # HDD, SSD
        - Flash Array
            # SSD
            # 특징
                ~ 데이터 중복제거(data reduction based on inline duplication)
                ~ 데이터 압축을 통한 공간 활용
                ~ 온라인 상태의 볼륨 볼륨 축소 가능(Scale-down)
                ~ IO 빈도수에 따른 hot/cold 데이터 tiering 가능(Disk or S3/Azure)

RAID(Redundant Array of Inexpensive/Independent Disk)
    * 무중단 구현(안정성) or 고성능 구현(효율성)을 목적으로 하는 디스크 사용 기술
    * RAID 단일 모드 종류
        - RAID 0(Stripe)
            # 최소 디스크 2개 이상
            # 구성 디스크에 데이터를 분산 배치
                ~ 1번 -> A, 2번 -> B, 3번 -> A...
            # R/W 성능 최상, 내결함성 X
                ~ 디스크 1개 장애시 데이터 복구 불가
        - RAID 1(Mirroring)
            # 최소 디스크 2개 이상
            # 구성 디스크의 데이터가 동일하게 복제됨(내결함성 O)
            # R 성능 준수, W 성능은 저조
        - RAID 5
            # 최소 디스크 3개 이상(권장 : 5개 이상)
                ~ 전체 용량 : (디스크 수-1) X 디스크 용량
            # RAID 0 방식 + 각 디스크에 장애 복구를 위한 패리티 정보 저장
                ~ 디스크 1개 장애시 데이터 남은 디스크들의 패리티 정보로 복구 가능
            # R/W 성능 준수, 내결함성 준수, 장애 복구시 성능 저하 발생
        - RAID 6
            # 최소 디스크 4개 이상
                ~ 전체 용량 : (디스크 수-2) X 디스크 용량
            # RAID 5 의 패리티 정보를 1개씩 추가 생성
            # R/W 성능 보통, 내결함성 상
    * RAID 혼합 모드 종류
        - RAID 10(1+0)
            # 최소 디스크 4개 이상
                ~ 전체 용량 : (디스크 수/2) X 디스크 용량
            # RAID 1 어레이 한 쌍이 RAID 0 구조로 구성
        - RAID 60(6+0)
            # 최소 디스크 8개 이상
                ~ 전체 용량 : (디스크 수-4) X 디스크 용량
            # R/W 성능 준수, 내결함성 상

플래시 스토리지
    * SSD 를 사용하는 스토리지 어레이
        - 기존의 원판과 엑추에이터로 R/W 를 하는 HDD 와 다르게 플래시 메모리로 구성
            # 전기를 통해 R/W 를 하기 때문에 HDD 보다 약 4배 정도 빠름

Thick Volume
    * 할당된 용량 중 미사용 용량까지 모두 스토리지 풀에서 사용 용량으로 확보
    * zeroing
        - 블록 공간 안의 데이터를 초기화하는 작업
            # lazy zeroing
                ~ write 작업 발생 시 zeroing 과정을 진행
                ~ zeroing 과정에서 로드 발생하여 write 성능 저하
            # eager zeroing
                ~ 사전에 zeroing 과정을 진행
Thin Volume(On-Demand Provisioning)
    * 가상으로 용량을 할당하고 실제 사용하는 용량만 스토리지 풀에서 사용 용량으로 확보
        - 가상화 기술을 통해 물리적 스토리지 풀 이상으로 공간 할당 가능
    * space reclamation
        - 볼륨 내 데이터가 삭제되어 미사용 용량이 되면 스토리지 풀로 반환 처리
            # 호스트에 부하 발생시킴

Static Volume
    * 기존에 존재하는 공간에서 볼륨을 생성하여 용량 확보
        - Thick, Thin 과 다르게 Snapshot, Tiering 기능 X

VPSA(Virtual Private Storage Array)
    * SAN(Storage Area Network)
        - 구성방식
            # FC(클라우드 인프라에서는 사용 X)
            # iSCSI(네트워크)
        - 특징
            # 빠름 & 고비용
                ~ DB, Tranaction
            # 공유 X(1:1 연결)
                ~ 로컬 호스트 연결(Volume Mount)
                ~ Cluster 솔루션을 통한 H/A 구현 시 1:2 가능
    * NAS(Network Attached Storage)
        - 구성방식
            # NFS
                ~ 리눅스 기반 OS
            # CIFS(SMB)
                ~ 윈도우 기반 OS
        - 특징
            # 공유 O(1:N 연결)
                ~ TCP/IP 기반의 인터넷 연결(Private)
    * Object-Storage
        - 구성방식
            # 느림 & 저비용
            # 객체에 대한 개별적 위치를 URL 로 제공
                ~ TCP/IP 기반의 인터넷 연결(Public)
            # API 기반

VPSA Creation Steps
1. RAID
    * RAID 1/6, Stripe-Size, Hot-Spare, Drive
        - 레이드 모드 및 블록 사이즈 선택(default : 64kb)
        - 예비 드라이브로 장애 드라이브 자동 교체 진행 여부 선택
        - 레이드 그룹에 속할 드라이브 개수 선택
    * Options
        - Media Scan
            # 레이드 그룹 내 패리티에 대한 데이터 무결성 체크 및 자동 보완
        - Force Recovery
            # 장애 드라이브에 대한 수동 복구 진행
                ~ Media Scan 이 선행되어야 함
        - Replacing Drive
            # 드라이브 교체
                ~ 하나의 SN(storage node)에서 2개 이상의 드라이브를 가져올 수 없음
        - Shredding Drive
            # 드라이브 내 데이터를 최소 3회 이상의 랜덤 데이터로 덮어서 지움
2. Pool
    * Array-Type, Cache, Striped
        - Storage/Flash/Object 타입 및 적합한 서비스 형태(Spec Diff.) 선택
            # Storage 는 상거래, 저장소, 아카이빙
            # Flash 는 IOPS/Throughput 최적화, 밸런스형
        - SSD 를 통한 캐싱 기능 선택
            # HDD 기반의 풀에서 만들어진 볼륨들의 성능 향상 기대
                ~ SSD 기반의 풀은 캐싱 기능 선택 X
        - 선택한 2개 이상의 레이드 그룹을 연동(RAID 0; Stripe) 선택
            # 성능 향상 기대
                ~ Flash Array 풀은 항상 연동되어 있어 선택 X
        - 레이드 그룹 or 드라이브를 선택하여 풀 생성 가능
            # 드라이브 선택 생성 시 레이드 그룹은 자동으로 생성
    * Option
        - Expanding
            # 온라인으로 풀 확장을 위해 용량 추가 가능
                ~ 레이드 그룹의 추가된 용량을 풀로 가져와서 확장
        - Shrinking
            # 온라인으로 풀 축소를 위해 용량 축소 가능(단, Flash Array 만 가능)
                ~ 미사용중인 레이드 그룹을 풀에서 삭제하여 용량 축소
                ~ 축소된 용량, 드라이브는 VPSA 로 반환되거나, VPSA 에서 탈장
        - Enable/Disable Caching
            # SSD 캐싱 기능을 토글 형식으로 활성화/비활성화 가능
                ~ HDD 가 캐싱 데이터에 의해 영향받으므로 부하량 상승 원인
                ~ Random IO 의 경우 캐싱에 의한 성능 상승 O
                  Sequencial IO 의 경우 성능 상승 X
        - Delete(Recycle bin)
            # 풀 내 볼륨 삭제 시 휴지통에서 7일동안 보관되며 이후 영구 삭제됨
                ~ 관리자가 수동으로 삭제/복원 가능
3. Volume
    * Block/NAS, Snapshot-Policy, SMB File-History
        - Block(iSCSI)
            # iSCSI 방식의 네트워크 연동을 통한 블록 볼륨
                ~ 용량, 풀, 암호화, 성능제한(IOPS, Throughput) 설정 가능
                ~ Flash 어레이의 경우, 압축(Compress), 중복제거(Dedupe) 옵션 추가 설정 가능
        - NAS Share(NFS/SMB)
            # NFS/SMB 방식의 네트워크 연동을 통한 파일시스템 볼륨
                ~ 용량, 풀, 노출
    * Snapshot-Policy
        - Cron 기반으로 사용자 정의 일정에 맞춰 스냅샷 생성 가능
            # 옵션에 따라 empty 스냅샷 생성 가능(이전 데이터와 동일할 경우 내용이 없는 빈 스냅샷)
    * SMB File-History
        - Windows OS 에서 NAS(SMB) 볼륨 내 파일에 대한 버전 관리 기능(스냅샷처럼 동작)
            # Snapshot-Policy 를 동일하게 사용함
4. Server
    * 볼륨 타입에 따라 제공받을 서버(클라이언트)에서 패키지, 방화벽(Port) 등 사전 작업 진행
        - Block(iSCSI)
            # Port : 3260/TCP
            # Linux
                ~ iscsi-initiator-util(iSCSI 블록 드라이브 연결을 위한 패키지)
                  sudo vi /etc/iscsi/initiatorname.iscsi -> 서버의 IQN 설정
                ~ iscsiadm -m discovery -t st -p <Active VC IP>
                  VC IQN 및 서비스포트 등 정보 확인 진행
                ~ iscsiadm -m node
                  iSCSI 타겟인 VPSA 정보 정상 여부 확인
            # Windows
                ~ 검색 창에 iSCSI 초기자 선택 후 서비스 시작
                  iSCSI 초기자 설정 > 구성 탭 -> 서버의 IQN 설정
        - NAS(NFS/SMB)
            # Port : 2049/TCP
            # Linux
                ~ nfs-utils(NFS 프로토콜을 통한 드라이브 연동 패키지)
    * ADD
        # Auto
            ~ Linux 는 제공되는 스크립트를 순차적으로 실행시키고,
              Windows 는 제공되는 파일을 실행
            ~ Refresh 를 통해 연결된 서버 확인
        # Manual
5. Attach & Mount
    * Resources > Volumes 에서 Servers 선택 후 Attach/Detach to Server 으로 볼륨 제공/제거
        - Block(iSCSI)
            # Linux 에서 lsscsi 를 통해 iSCSI target 으로부터 확인된 블록 정보 확인
                ~ fdisk 등을 통해 확인된 블록을 초기화
                ~ mkfs 를 통해 파일시스템(ext4) 설정
                ~ 마운트 포인트(디렉토리) 생성 및 마운트
                ~ /etc/fstab 에 해당 iSCSI 마운트 정보를 입력하여 부팅 시 자동 마운트 설정
                  부팅 옵션으로 _netdev 로 설정하여 네트워크 연결 후 마운트 순서 설정
            # Windows 에서 iSCSI 초기자 설정 > 볼륨 및 장치 > 자동 구성 선택
                ~ 실행 > diskmgmt.msc
                ~ 디스크 온라인 및 초기화 진행 후 볼륨 설정하여 마운트
                ~ 해당 볼륨에 대해 적절한 권한 설정 후 사용
        - NAS(NFS)
            # Linux 에서 마운트 포인트 생성 후 마운트
                ~ mount -t nfs4 <VPSA NFS 외부 경로> <마운트 포인트 경로> 명령어로
                  파일시스템 타입까지 한번에 설정 가능
                ~ etc/fstab 에 해당 NFS 마운트 정보를 입력하여 부팅 시 자동 마운트 설정
                  부팅 옵션으로 _netdev 지정하여 네트워크 연결 후 마운트 순서 지정
        - NAS(SMB|CIFS)
            # Windows 에서 내 컴퓨터 우클릭 > 네트워크 드라이브 연결.. 선택
                ~ 드라이브 문자 선택 후 VPSA SMB 외부 경로 입력
                ~ VPSA NAS User 에 등록된 사용자 정보로 로그인
                ~ Guest Access 허용 시 Anonymous 형태로 로그인 가능
                  하지만 윈도우 로컬 정책에서 게스트 로그인 정책 사용 설정해줘야 가능
                  gpedit.msc > 컴퓨터 관리템플릿 > 네트워크 > lanman 워크스테이션 > 설정 확인

Monitoring
    * 스토리지 주요 성능 관제 포인트
        - IOPS
            # 초 당 입출력 프로세스율
                ~ 속도
        - Throughput
            # 프로세스 처리량
                ~ 파워
        - Latency
            # Volume
                ~ 볼륨에 대한 지연율은 볼륨과 관련되어있는 각각의 리소스에 영향을 받으므로 상대적인 지표
            # Disk
                ~ 디스크 지연율은 실제 스토리지의 성능을 직관적으로 확인해볼 수 있는 지표
                ~ 대략적으로 HDD 는 50~60 ms, SSD 는 10 ms 미만의 지연율이 확보되야 보통
            # CPU/MEM(VC; Engine)
                ~ 시스템 리소스의 경우, 볼륨/디스크의 지연 상관성을 확인해볼 수 있는 지표

Troubleshooting
    * HDD Latency
        - 드라이브 추가를 통해 스핀들(디스크 작업 성능) 상승 기대
            # RAID 0(Stripe)
        - SSD 드라이브로 교체하여 베이스 성능 상승 기대
    * iSCSI 리눅스 연동해제
        - iscsiadm -m node -T <VPSA IQN> -p <Active VC IP> -u(logout) 진행
            # 정상적으로 진행이 안될 경우(database failure 등)
                ~ 5~10 분 정도 대기 후 로그아웃, 타겟 디스커버리, 로그인/로그아웃 과정 재진행 요망