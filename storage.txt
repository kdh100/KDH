Zadara Storage(VPSA)
    * 사용량 기반 요금 정책의 클라우드 형태 서비스(STaaS; Storage as a Service)
        - 각 서비스 당 최대 100개의 노드, 64 PB 용량 운용 가능
        - 물리적 스토리지 장비를 통합하여 가상 스토리지 장비를 생성
            # 고객 별 가상 스토리지의 스펙 조정 가능(CPU, MEM 등)
                ~ IOPS 효율성 및 최대화 기대 가능
            # H/A 기능 및 Control Core(Engine) 이중화(A-S) 기능 구현
    * 클라우드 형태 서비스
        - 단독(직연동; Direct) 클라우드 및 멀티 클라우드 연결 지원
            # B2C 부터 다중 CSP <-> 클라우드 간 연결 가능
    * 장비 스펙
        - 볼륨 당 최대 150,000 IOPS
        - Snapshot-Clone, Mirroring, Online-Migration 지원
    * 장비군
        - Storage Array
            # HDD, SSD
        - Flash Array
            # SSD
            # 특징
                ~ 데이터 중복제거(data reduction based on inline duplication)
                ~ 데이터 압축을 통한 공간 활용
                ~ 온라인 상태의 볼륨 볼륨 축소 가능(Scale-down)
                ~ IO 빈도수에 따른 hot/cold 데이터 tiering 가능(Disk or S3/Azure)

RAID(Redundant Array of Inexpensive/Independent Disk)
    * 무중단 구현(안정성) or 고성능 구현(효율성)을 목적으로 하는 디스크 사용 기술
    * RAID 단일 모드 종류
        - RAID 0(Stripe)
            # 최소 디스크 2개 이상
            # 구성 디스크에 데이터를 분산 배치
                ~ 1번 -> A, 2번 -> B, 3번 -> A...
            # R/W 성능 최상, 내결함성 X
                ~ 디스크 1개 장애시 데이터 복구 불가
        - RAID 1(Mirroring)
            # 최소 디스크 2개 이상
            # 구성 디스크의 데이터가 동일하게 복제됨(내결함성 O)
            # R 성능 준수, W 성능은 저조
        - RAID 5
            # 최소 디스크 3개 이상(권장 : 5개 이상)
                ~ 전체 용량 : (디스크 수-1) X 디스크 용량
            # RAID 0 방식 + 각 디스크에 장애 복구를 위한 패리티 정보 저장
                ~ 디스크 1개 장애시 데이터 남은 디스크들의 패리티 정보로 복구 가능
            # R/W 성능 준수, 내결함성 준수, 장애 복구시 성능 저하 발생
        - RAID 6
            # 최소 디스크 4개 이상
                ~ 전체 용량 : (디스크 수-2) X 디스크 용량
            # RAID 5 의 패리티 정보를 1개씩 추가 생성
            # R/W 성능 보통, 내결함성 상
    * RAID 혼합 모드 종류
        - RAID 10(1+0)
            # 최소 디스크 4개 이상
                ~ 전체 용량 : (디스크 수/2) X 디스크 용량
            # RAID 1 어레이 한 쌍이 RAID 0 구조로 구성
        - RAID 60(6+0)
            # 최소 디스크 8개 이상
                ~ 전체 용량 : (디스크 수-4) X 디스크 용량
            # R/W 성능 준수, 내결함성 상

플래시 스토리지
    * SSD 를 사용하는 스토리지 어레이
        - 기존의 원판과 엑추에이터로 R/W 를 하는 HDD 와 다르게 플래시 메모리로 구성
            # 전기를 통해 R/W 를 하기 때문에 HDD 보다 약 4배 정도 빠름

Thick Volume
    * 할당된 용량 중 미사용 용량까지 모두 스토리지 풀에서 사용 용량으로 확보
    * zeroing
        - 블록 공간 안의 데이터를 초기화하는 작업
            # lazy zeroing
                ~ write 작업 발생 시 zeroing 과정을 진행
                ~ zeroing 과정에서 로드 발생하여 write 성능 저하
            # eager zeroing
                ~ 사전에 zeroing 과정을 진행
Thin Volume(On-Demand Provisioning)
    * 가상으로 용량을 할당하고 실제 사용하는 용량만 스토리지 풀에서 사용 용량으로 확보
        - 가상화 기술을 통해 물리적 스토리지 풀 이상으로 공간 할당 가능
    * space reclamation
        - 볼륨 내 데이터가 삭제되어 미사용 용량이 되면 스토리지 풀로 반환 처리
            # 호스트에 부하 발생시킴
Static Volume
    * 기존에 존재하는 공간에서 볼륨을 생성하여 용량 확보
        - Thick, Thin 과 다르게 Snapshot, Tiering 기능 X

VPSA(Virtual Private Storage Array)
    * SAN(Storage Area Network)
        - 구성방식
            # FC(클라우드 인프라에서는 사용 X)
            # iSCSI(네트워크)
        - 특징
            # 빠름 & 고비용
                ~ DB, Tranaction
            # 공유 X(1:1 연결)
                ~ 로컬 호스트 연결(Volume Mount)
                ~ Cluster 솔루션을 통한 H/A 구현 시 1:2 가능
    * NAS(Network Attached Storage)
        - 구성방식
            # NFS
                ~ 리눅스 기반 OS
            # CIFS(SMB)
                ~ 윈도우 기반 OS
        - 특징
            # 공유 O(1:N 연결)
                ~ TCP/IP 기반의 인터넷 연결(Private)
    * Object-Storage
        - 구성방식
            # 느림 & 저비용
            # 객체에 대한 개별적 위치를 URL 로 제공
                ~ TCP/IP 기반의 인터넷 연결(Public)
            # API 기반

VPSA Creation Steps
1. RAID
    * RAID 1/6, Stripe-Size, Hot-Spare, Drive
        - 레이드 모드 및 블록 사이즈 선택(default : 64kb)
        - 예비 드라이브로 장애 드라이브 자동 교체 진행 여부 선택
        - 레이드 그룹에 속할 드라이브 개수 선택
    * Options
        - Media Scan
            # 레이드 그룹 내 패리티에 대한 데이터 무결성 체크 및 자동 보완
        - Force Recovery
            # 장애 드라이브에 대한 수동 복구 진행
                ~ Media Scan 이 선행되어야 함
        - Replacing Drive
            # 드라이브 교체
                ~ 하나의 SN(storage node)에서 2개 이상의 드라이브를 가져올 수 없음
        - Shredding Drive
            # 드라이브 내 데이터를 최소 3회 이상의 랜덤 데이터로 덮어서 지움
2. Pool
    * Array-Type, Cache, Striped
        - Storage/Flash/Object 타입 및 적합한 서비스 형태(Spec Diff.) 선택
            # Storage 는 상거래, 저장소, 아카이빙
            # Flash 는 IOPS/Throughput 최적화, 밸런스형
        - SSD 를 통한 캐싱 기능 선택
            # HDD 기반의 풀에서 만들어진 볼륨들의 성능 향상 기대
                ~ SSD 기반의 풀은 캐싱 기능 선택 X
        - 선택한 2개 이상의 레이드 그룹을 연동(RAID 0; Stripe) 선택
            # 성능 향상 기대
                ~ Flash Array 풀은 항상 연동되어 있어 선택 X
        - 레이드 그룹 or 드라이브를 선택하여 풀 생성 가능
            # 드라이브 선택 생성 시 레이드 그룹은 자동으로 생성
    * Option
        - Expanding
            # 온라인으로 풀 확장을 위해 용량 추가 가능
                ~ 레이드 그룹의 추가된 용량을 풀로 가져와서 확장
        - Shrinking
            # 온라인으로 풀 축소를 위해 용량 축소 가능(단, Flash Array 만 가능)
                ~ 미사용중인 레이드 그룹을 풀에서 삭제하여 용량 축소
                ~ 축소된 용량, 드라이브는 VPSA 로 반환되거나, VPSA 에서 탈장
        - Enable/Disable Caching
            # SSD 캐싱 기능을 토글 형식으로 활성화/비활성화 가능
                ~ HDD 가 캐싱 데이터에 의해 영향받으므로 부하량 상승 원인
                ~ Random IO 의 경우 캐싱에 의한 성능 상승 O
                  Sequencial IO 의 경우 성능 상승 X
        - Delete(Recycle bin)
            # 풀 내 볼륨 삭제 시 휴지통에서 7일동안 보관되며 이후 영구 삭제됨
                ~ 관리자가 수동으로 삭제/복원 가능
3. Volume
    * Block/NAS, Snapshot-Policy, SMB File-History
        - Block(iSCSI)
            # iSCSI 방식의 네트워크 연동을 통한 블록 볼륨
                ~ 용량, 풀, 암호화, 성능제한(IOPS, Throughput) 설정 가능
                ~ Flash 어레이의 경우, 압축(Compress), 중복제거(Dedupe) 옵션 추가 설정 가능
        - NAS Share(NFS/SMB)
            # NFS/SMB 방식의 네트워크 연동을 통한 파일시스템 볼륨
                ~ 용량, 풀, 노출
    * Snapshot-Policy
        - Cron 기반으로 사용자 정의 일정에 맞춰 스냅샷 생성 가능
            # 옵션에 따라 empty 스냅샷 생성 가능(이전 데이터와 동일할 경우 내용이 없는 빈 스냅샷)
    * SMB File-History
        - Windows OS 에서 NAS(SMB) 볼륨 내 파일에 대한 버전 관리 기능(스냅샷처럼 동작)
            # Snapshot-Policy 를 동일하게 사용함
4. Server
    * 볼륨 타입에 따라 제공받을 서버(클라이언트)에서 패키지, 방화벽(Port) 등 사전 작업 진행
        - Block(iSCSI)
            # Port : 3260/TCP
            # Linux
                ~ iscsi-initiator-util(iSCSI 블록 드라이브 연결을 위한 패키지)
                  sudo vi /etc/iscsi/initiatorname.iscsi -> 서버의 IQN 설정
                ~ iscsiadm -m discovery -t st -p <Active VC IP>
                  VC IQN 및 서비스포트 등 정보 확인 진행
                ~ iscsiadm -m node
                  iSCSI 타겟인 VPSA 정보 정상 여부 확인
                ~ iscsiadm -m node -T <VC IQN> -p <VC IP> -o update \
                  -n node.session.authmethod -v CHAP \
                  -n node.session.username -v <VPSA CHAP ID> \
                  -n node.session.password -v <VPSA CHAP PW>
                  VC 에 대한 CHAP 방식 인증 정보 업데이트 진행
            # Windows
                ~ 검색 창에 iSCSI 초기자 선택 후 서비스 시작
                  iSCSI 초기자 설정 > 구성 탭 -> 서버의 IQN 설정
        - NAS(NFS/SMB)
            # Port : 2049/TCP
            # Linux
                ~ nfs-utils(NFS 프로토콜을 통한 드라이브 연동 패키지)
    * ADD
        # Auto
            ~ Linux 는 제공되는 스크립트를 순차적으로 실행시키고,
              Windows 는 제공되는 파일을 실행
            ~ Refresh 를 통해 연결된 서버 확인
        # Manual
5. Attach & Mount
    * Resources > Volumes 에서 Servers 선택 후 Attach/Detach to Server 으로 볼륨 제공/제거
        - Block(iSCSI)
            # Linux 에서 lsscsi --scsi_id 를 통해 iSCSI target 으로부터 확인된 블록 정보 확인
                ~ parted(2TB 이상), fdisk(2TB 미만) 등을 통해 확인된 블록을 초기화
                ~ mkfs 를 통해 파일시스템(ext4) 설정
                ~ 마운트 포인트(디렉토리) 생성 및 마운트
                ~ /etc/fstab 에 해당 iSCSI 마운트 정보를 입력하여 부팅 시 자동 마운트 설정
                  부팅 옵션으로 _netdev 로 설정하여 네트워크 연결 후 마운트 순서 설정
            # Windows 에서 iSCSI 초기자 설정 > 볼륨 및 장치 > 자동 구성 선택
                ~ 실행 > diskmgmt.msc
                ~ 디스크 온라인 및 초기화 진행 후 볼륨 설정하여 마운트
                ~ 해당 볼륨에 대해 적절한 권한 설정 후 사용
        - NAS(NFS)
            # Linux 에서 마운트 포인트 생성 후 마운트
                ~ mount -t nfs4 <VPSA NFS 외부 경로> <마운트 포인트 경로> 명령어로
                  파일시스템 타입까지 한번에 설정 가능
                ~ etc/fstab 에 해당 NFS 마운트 정보를 입력하여 부팅 시 자동 마운트 설정
                  부팅 옵션으로 _netdev 지정하여 네트워크 연결 후 마운트 순서 지정
        - NAS(SMB|CIFS)
            # Windows 에서 내 컴퓨터 우클릭 > 네트워크 드라이브 연결.. 선택
                ~ 드라이브 문자 선택 후 VPSA SMB 외부 경로 입력
                ~ VPSA NAS User 에 등록된 사용자 정보로 로그인
                ~ Guest Access 허용 시 Anonymous 형태로 로그인 가능
                  하지만 윈도우 로컬 정책에서 게스트 로그인 정책 사용 설정해줘야 가능
                  gpedit.msc > 컴퓨터 관리템플릿 > 네트워크 > lanman 워크스테이션 > 설정 확인
6. Expand
    * Resource > Volumes 에서 Expand 선택 후 용량 증가
        - Block(iSCSI)
            # Linux 에서 lsblk, df -h 를 통해서 추가된 볼륨 용량 확인
                ~ sudo growpart(용량 확장 패키지) <iSCSI 볼륨> <파티션번호> 으로 용량 확장
                ~ sudo { resize2fs | xfs_growfx } <iSCSI 볼륨 파티션> 으로 파일시스템 확장(resizing)
            # Windows 에서 디스크 관리에서 추가된 볼륨 용량 확인
                ~ 디스크 볼륨에 대한 미사용 용량 확인 후 사용중인 볼륨 우클릭 > 확장 진행

Monitoring
    * 스토리지 주요 성능 관제 포인트
        - IOPS
            # 초 당 입출력 프로세스율
                ~ 속도
        - Throughput
            # 프로세스 처리량
                ~ 파워
        - Latency
            # Volume
                ~ 볼륨에 대한 지연율은 볼륨과 관련되어있는 각각의 리소스에 영향을 받으므로 상대적인 지표
            # Disk
                ~ 디스크 지연율은 실제 스토리지의 성능을 직관적으로 확인해볼 수 있는 지표
                ~ 대략적으로 HDD 는 50~60 ms, SSD 는 10 ms 미만의 지연율이 확보되야 보통
            # CPU/MEM(VC; Engine)
                ~ 시스템 리소스의 경우, 볼륨/디스크의 지연 상관성을 확인해볼 수 있는 지표

Troubleshooting
공통
    * HDD Latency
        - 드라이브 추가를 통해 스핀들(디스크 작업 성능) 상승 기대
            # RAID 0(Stripe)
        - SSD 드라이브로 교체하여 베이스 성능 상승 기대
리눅스
    * iSCSI 리눅스 연동해제
        - iscsiadm -m node -T <VPSA IQN> -p <Active VC IP> -u(logout) 진행
            # 정상적으로 진행이 안될 경우(database failure 등)
                ~ 5~10 분 정도 대기 후 로그아웃, 타겟 디스커버리, 로그인/로그아웃 과정 재진행 요망
    * iSCSI 볼륨 탈장 후 lsscsi 에 남아있는 디스크 정보 삭제
        - 기본적으로 리부팅하면 블록 정보를 rescan 해서 정리할 수 있으나 권장되지는 않음
        - su - //루트 권한 필요
          echo 1 >/sys/block/<제거할 iscsi 디스크>/device/delete
        - lsscsi 로 제거 확인
    * NAS 연동했던 마운트포인트 ???? 표시로 삭제할 수 없을 때
        - sudo umount <디렉토리> 후에 sudo rm -rf <디렉토리> 진행
윈도우
    * 연동된 NAS 드라이브를 관리자 권한 프로그램을 통해서 경로가 확인되지 않는 경우
        - UAC(User Access Control; 윈도우 사용자 계정 컨트롤) 이 켜져있는 환경에서 사용자/관리자 세션이
          네트워크 드라이브에 대한 영역은 공유하지 않기 때문에 발생하는 문제
            # CMD 창을 관리자 권한/사용자 권한으로 열어 'net use' 를 통해 공유가 안되는 현상 확인 가능
            # 실행 > regedit > 
              HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Policies\System 선택 후
              빈칸에 우클릭 > DWORD(32비트) 생성 > 'EnableLinkedConnections' 생성 및 값 1로 수정
            # 리부팅 후 현상 재확인

Service
KT 천안CDC -> CloudStack 기반
    * Public Zone
        - KT Central A
        - KT Central B
        - KT M1(실 사용 고객은 목동, 장비만 천안)
    * G Cloud(공공)
        - KT G-G1
            # DMZ, Private
        - KT G-D1
            # Zadara Object Storage(Public-Service)
        - KT G-KIPO(특허청) -> OpenStack 기반
KT 목동 -> OpenStack 기반
    * 1센터
        - KT D1
    * 2센터
        - KT M2
            # Zadara Object Storage(Public-Service)

Network

Reference
리눅스
    * LVM 생성
        - sudo yum install -y lvm2
        - pvcreate /dev/sda1
        - pvs
        - vgcreate <vg 이름> /dev/sda1
        - vgs
        - lvcreate -n <lv 이름> -l +100%FREE <vg 이름>
            # -n : 이름
              -l : 사이즈 퍼센트 지정
              -L : 사이즈 단위 지정(K,M,G,T)
        - lvs
        - mkfs -t <fs 타입> /dev/<vg 이름>/<lv 이름>
        - mount /dev/<vg 이름>/<lv 이름> <마운트포인트>
    * LVM 확장/감소
        - lsblk, df 를 통한 추가 디스크 확인 및 fdisk 를 통한 초기화
        - pvcreate /dev/sdb1
        - pvs
        - vgextend | vgreduce <vg 이름> /dev/sdb1
        - vgs
        - lvextend -l +100%FREE | lvreduce -L -500G /dev/<vg 이름>/<lv 이름>
        - lvs
        - resize2fs | xfs_growfx /dev/<vg 이름>/<lv 이름>
            # resize2fs : ext4
              xfs_growfx : xfs
    * MPIO(Multipath)
        - 다중 세션에 의한 IO 경로 확장으로 디스크 성능 overload 가능(세션이 너무 많을 경우 효용성 감소)
        - yum install -y device-mapper-multipath
        - systemctl enable --now multipathd
        - vi /etc/multipath.conf  //다중경로 설정파일
        ==============================================
        defaults {  //전체 다중경로에 대한 기본 옵션
          checker_timeout 600
          user_friendly_names yes  //alias 사용
        }
        blacklist {  //다중경로에 해당되지 않는 경우 정의
          device {
            vendor "QEMU"
            product '*'
          }
        }
        # The below section will handle Zadara volumes
        devices {
          device {  //다중경로 해당되는 장비별 정의
            vendor "Zadara"
            product "VPSA"
         path_grouping_policy multibus
            path_checker tur
         # You can try different path selectors
            path_selector "round-robin 0"
            # path_selector "queue-length 0"
            # path_selector "service-time 0"
         failback manual
         rr_min_io 1
            no_path_retry 20
          }
        }
        multipaths {  //다중경로에 대한 특성 설정
          multipath {
            wwid 23465623261653636  //다중경로 내 WWID 에 대한 지정
            alias miscsi1  //지정된 WWID 에 대한 alias 지정, lsblk 에서 해당 alias 명으로 확인
          }
        }
        ==============================================